# 5. Split as per best identified criterion.
# Split the data as per split condition.
split_var_dtype <- get_cat_con(data[[split_variable]])
data_split <- hash()
if (split_var_dtype == 'con') {
data_split[['left']] <- data[
data[[split_variable]] < split_set,
, drop = FALSE
]
data_split[['right']] <- data[
data[[split_variable]] >= split_set,
, drop = FALSE
]
} else {
data_split[['left']] <- data[
data[[split_variable]] %in% split_set,
, drop = FALSE
]
data_split[['right']] <- data[
!data[[split_variable]] %in% split_set,
, drop = FALSE
]
}
# 6. Recursively call the create tree
#    function with data from both child
#    nodes to build the next level of the tree.
left_child <- create_tree(
data = data_split[['left']],
level=level+1
)
right_child <- create_tree(
data = data_split[['right']],
level=level+1
)
# Return intermediate node.
node <- hash()
node[['type']] <- "intermediate"
node[['split_variable']] <- split_variable
node[['split_set']] <- split_set
node[['model_type']] <- model_type
node[['left_child']] <- left_child
node[['right_child']] <- right_child
return(node)
}
# MAIN
# Load data.
school_absences <- read.csv(
"../Data/data_clean.csv",
header=TRUE, sep=","
)
RESPONSE_VARIABLE <- "absences"
# Shuffle
set.seed(32)
shuffled_data <- school_absences[
sample(nrow(school_absences)),
]
# Reset index.
rownames(shuffled_data) <- NULL
# 80/20 train/test split.
index_train <- createDataPartition(
shuffled_data[RESPONSE_VARIABLE][,],
p = 0.8, list = FALSE
)
data_train <- shuffled_data[index_train, ]
data_test <- shuffled_data[-index_train, ]
# Sanitize data for model fitting.
data_sanitized <- sanitize_fit_input(
data_check = data_train,
data_apply = list(data_train, data_test),
response_variable = RESPONSE_VARIABLE
)
data_train <- data_sanitized[[1]]
data_test <- data_sanitized[[2]]
# Build tree.
TOTAL_N_ROWS <- nrow(data_train) # global variable.
root <- create_tree(data = data_train)
# LOAD PACKAGES
require(pscl) # Zero inflated poisson model.
require(MASS) # Poisson and negative binomial models.
require(caret) # For cross validation.
require(rstudioapi)
# SET WORKING DIRECTORY
current_path = rstudioapi::getActiveDocumentContext()$path
print(current_path)
setwd(dirname(current_path ))
print( getwd() )
# LOAD PACKAGES
require(pscl) # Zero inflated poisson model.
require(MASS) # Poisson and negative binomial models.
require(caret) # For cross validation.
require(rstudioapi) # For dynamic setwd(...)
# SET WORKING DIRECTORY
current_path = rstudioapi::getActiveDocumentContext()$path
print(current_path)
setwd(dirname(current_path ))
print( getwd() )
# HELPER FUNCTIONS
source("./helper_functions.R")
cv10fold <- function(
model_type, data, response_variable
) {
### Returns average MSE after 10 fold cross validation.
### @param response_variable: Variable to predict.
### @param model_type: Code of the type of
###                    model to use.
### @param data: Data to perform 10 fold CV using.
### @return: Average of MSE values across 10 folds.
folds <- 10
n <- nrow(data)
fold_indices <- cut(
1:n, breaks = folds,
labels = FALSE
)
# Initialize for accumulating errors.
total_error <- 0
for (i in 1:folds) {
fold_train <- data[fold_indices != i,]
fold_validate <- data[fold_indices == i,]
data_sanitized <- sanitize_fit_input(
data_check = fold_train,
data_apply = list(fold_train, fold_validate),
response_variable = response_variable
)
fold_train <- data_sanitized[[1]]
fold_validate <- data_sanitized[[2]]
# Fit model on train set.
m <- fit_model(
model_type = model_type,
data = fold_train,
response_variable = response_variable
)
# Make predictions on validation set.
y <- fold_validate[[response_variable]]
y_pred <- predict(
m, newdata = fold_validate,
type = "response"
)
# Calculate squared error.
error <- sum((y - y_pred)^2)
total_error <- total_error + error
}
# Calculate and return mean squared error (MSE).
mse <- total_error / n
return(mse)
}
# MODEL SELECTION
get_best_model <- function(data, response_variable) {
### Returns best model. The best model is the
### one that resulted in lowest MSE after
### 10 fold cross validation (CV).
### @param data: Input data set.
### @param response_variable: Variable to predict.
### @return: Code indicating best model so identified.
print('Determining best model through 10 Fold CV ...')
model_types <- list(
'p', 'nb', 'zip', 'zinb', 'hp', 'hnb'
)
res_cv <- list()
for (model_type in model_types) {
mse <- tryCatch({cv10fold(
model_type = model_type, data = data,
response_variable = response_variable
)}, error = function(e) {
print(paste(
"Model", model_type, "cannot be fit.",
"Thus, no longer a candidate for best model."
))
return(Inf)
})
res_cv <- append(res_cv, mse)
print(paste(
'Evaluated model:', model_type,
'(mse =', mse, ")"
))
}
if (min(res_cv) == Inf) {
return('none')
}
model_best <- model_types[which.min(res_cv)]
print(paste(
'Model resulting in lowest avg.',
'MSE across 10 folds =', model_best
))
return(model_best)
}
# Load data.
school_absences <- read.csv(
"../Data/data_clean.csv",
header=TRUE, sep=","
)
response_variable <- "absences"
# Get best model.
model_type <- get_best_model(
data = school_absences,
response_variable = response_variable
)
res_cv
res_cv <- list()
for (model_type in model_types) {
mse <- tryCatch({cv10fold(
model_type = model_type, data = data,
response_variable = response_variable
)}, error = function(e) {
print(paste(
"Model", model_type, "cannot be fit.",
"Thus, no longer a candidate for best model."
))
return(Inf)
})
res_cv <- append(res_cv, mse)
print(paste(
'Evaluated model:', model_type,
'(mse =', mse, ")"
))
}
model_types <- list(
'p', 'nb', 'zip', 'zinb', 'hp', 'hnb'
)
res_cv <- list()
for (model_type in model_types) {
mse <- tryCatch({cv10fold(
model_type = model_type, data = data,
response_variable = response_variable
)}, error = function(e) {
print(paste(
"Model", model_type, "cannot be fit.",
"Thus, no longer a candidate for best model."
))
return(Inf)
})
res_cv <- append(res_cv, mse)
print(paste(
'Evaluated model:', model_type,
'(mse =', mse, ")"
))
}
res_cv
min(res_cv)
min(res_cv[,])
min(delist(res_cv))
min(unlist(res_cv))
which.min(res_cv)
# MODEL SELECTION
get_best_model <- function(data, response_variable) {
### Returns best model. The best model is the
### one that resulted in lowest MSE after
### 10 fold cross validation (CV).
### @param data: Input data set.
### @param response_variable: Variable to predict.
### @return: Code indicating best model so identified.
print('Determining best model through 10 Fold CV ...')
model_types <- list(
'p', 'nb', 'zip', 'zinb', 'hp', 'hnb'
)
res_cv <- list()
for (model_type in model_types) {
mse <- tryCatch({cv10fold(
model_type = model_type, data = data,
response_variable = response_variable
)}, error = function(e) {
print(paste(
"Model", model_type, "cannot be fit.",
"Thus, no longer a candidate for best model."
))
return(Inf)
})
res_cv <- append(res_cv, mse)
print(paste(
'Evaluated model:', model_type,
'(mse =', mse, ")"
))
}
if (min(unlist(res_cv)) == Inf) {
return('none')
}
model_best <- model_types[which.min(res_cv)]
print(paste(
'Model resulting in lowest avg.',
'MSE across 10 folds =', model_best
))
return(model_best)
}
# Get best model.
model_type <- get_best_model(
data = school_absences,
response_variable = response_variable
)
# LOAD PACKAGES
require(hash) # For hash map data structure.
require(rstudioapi) # For dynamic setwd(...)
# SET WORKING DIRECTORY
current_path = rstudioapi::getActiveDocumentContext()$path
print(current_path)
setwd(dirname(current_path ))
# LOAD PACKAGES
require(hash) # For hash map data structure.
require(rstudioapi) # For dynamic setwd(...)
# SET WORKING DIRECTORY
current_path = rstudioapi::getActiveDocumentContext()$path
print(current_path)
setwd(dirname(current_path ))
print(getwd())
# HELPER FUNCTIONS
source("./helper_functions.R")
source("./model_selection.R")
source("./split_variable_selection.R")
source("./split_set_selection.R")
create_tree <- function(data, level=0) {
### A function that recursively builds a decision tree.
### @param data: Data set using which to build the tree.
### @param RESPONSE_VARIABLE: The variable that is to be
###                           predicted.
### @return: Terminal or intermediate nodes of the tree
###          with information such as given below.
###          Here, $ => for intermediate node only and
###                £ => for terminal node only.
###          * Node type
###          * Data £
###          * Model type $
###          * Split variable $
###          * Split set $
###          * Left child $
###          * Right child $
print(paste(
"Building Tree ( level", level, ")"
))
# 1. Model selection.
model_type <- get_best_model(
data = data,
response_variable = RESPONSE_VARIABLE
)
# 2. Check for stopping condition.
# If no model could fit this data, then this
# is an invalid leaf that will be pruned.
if(model_type == 'none') {
# Return terminal node.
node <- hash()
node[["type"]] <- "invalid"
node[['data']] <- data
return(node)
}
# Stop growing if this node has < 5% of
# the data points in the original data set
# or if all values in the response variable
# are the same. This is a valid leaf.
if (
nrow(data) <= (0.05 * TOTAL_N_ROWS) ||
length(unique(data[[RESPONSE_VARIABLE]])) == 1
) {
# Return terminal node.
node <- hash()
node[["type"]] <- "terminal"
node[['data']] <- data
node[['model_type']] <- model_type
node[['has_left_child']] <- FALSE
node[['has_right_child']] <- FALSE
return(node)
}
# 3. Split variable selection.
split_variable <- get_split_variable(
data = data,
response_variable = RESPONSE_VARIABLE,
model_type = model_type
)
# 4. Split set selection.
split_set <- get_split_set(
split_variable = split_variable,
data = data,
model_type = model_type,
response_variable = RESPONSE_VARIABLE
)
# 5. Split as per best identified criterion.
# Split the data as per split condition.
split_var_dtype <- get_cat_con(data[[split_variable]])
data_split <- hash()
if (split_var_dtype == 'con') {
data_split[['left']] <- data[
data[[split_variable]] < split_set,
, drop = FALSE
]
data_split[['right']] <- data[
data[[split_variable]] >= split_set,
, drop = FALSE
]
} else {
data_split[['left']] <- data[
data[[split_variable]] %in% split_set,
, drop = FALSE
]
data_split[['right']] <- data[
!data[[split_variable]] %in% split_set,
, drop = FALSE
]
}
# 6. Recursively call the create tree
#    function with data from both child
#    nodes to build the next level of the tree.
left_child <- create_tree(
data = data_split[['left']],
level=level+1
)
right_child <- create_tree(
data = data_split[['right']],
level=level+1
)
# Return intermediate node.
node <- hash()
node[['type']] <- "intermediate"
node[['split_variable']] <- split_variable
node[['split_set']] <- split_set
node[['model_type']] <- model_type
if (!left_child[['type']] == 'invalid') {
node[['left_child']] <- left_child
node[['has_left_child']] <- TRUE
} else {
node[['has_left_child']] <- FALSE
}
if (!right_child[['type']] == 'invalid') {
node[['right_child']] <- right_child
node[['has_right_child']] <- TRUE
} else {
node[['has_right_child']] <- FALSE
}
if (
node[['has_left_child']] == FALSE &&
node[['has_right_child']] == FALSE
) {
node[['type']] <- "terminal"
}
return(node)
}
# MAIN
# Load data.
school_absences <- read.csv(
"../Data/data_clean.csv",
header=TRUE, sep=","
)
RESPONSE_VARIABLE <- "absences"
# Shuffle
set.seed(32)
shuffled_data <- school_absences[
sample(nrow(school_absences)),
]
# Reset index.
rownames(shuffled_data) <- NULL
# 80/20 train/test split.
index_train <- createDataPartition(
shuffled_data[RESPONSE_VARIABLE][,],
p = 0.8, list = FALSE
)
data_train <- shuffled_data[index_train, ]
data_test <- shuffled_data[-index_train, ]
# Sanitize data for model fitting.
data_sanitized <- sanitize_fit_input(
data_check = data_train,
data_apply = list(data_train, data_test),
response_variable = RESPONSE_VARIABLE
)
data_train <- data_sanitized[[1]]
data_test <- data_sanitized[[2]]
# Build tree.
TOTAL_N_ROWS <- nrow(data_train) # global variable.
root <- create_tree(data = data_train)
source("~/TCD/Modules/CS7DS1_DataAnalytics/Project/Code/tree_building.R")
root
names(school_absences)
# Import libraries.
library(dplyr)
library(caret) # For label encoding.
library(ggplot2)
# GETTING DATA
# Set working directory to this one.
setwd("C:/Users/g_gna/Documents/TCD/Modules/CS7DS1_DataAnalytics/Project/Data")
# Load and view data.
data <- read.csv("./data.csv", header=TRUE, sep= ",")
str(data)
# Extract useful features only and view the data.
data <- as.data.frame(data[, c('absences', 'course', 'school', 'sex', 'age', 'address', 'famsize', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'activities', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'G3', 'nursery')])
head(data)
# EDA
# Check length of columns.
for (column in names(data)) {
print(paste(column, "= ", length(data[, c(column)])))
}
# Obs. All same length.
# Check for NaN values.
num_rows_na <- sum(rowSums(is.na(data)) > 0) # 0
num_rows_not_na <- nrow(data) - num_rows_na # 1044
# Obs. No missing values.
# Examining feature category value counts.
for (col in names(data)) {
print(paste("Value Counts - Column '", col, "'"))
print(as.data.frame(table(data[col])))
cat("\n")
}
# OBS. All good.
# Blank value handling.
num_rows_blank <- sum(rowSums(data == "" | data == " ") > 0) # 0
num_rows_not_blank <- nrow(data) - num_rows_blank # 0
# Obs. No blank values.
# View data.
print(summary(data$absences)) # Min = 0, Q1 = 0, Median = 2, Mean = 4.435, Q3 = 6, Max = 75
print(paste("Variance:", var(data$absences))) # Variance = 38.564
print(paste("No. of 0 counts:", sum(data$absences == 0))) # 359
print(paste("No. of non 0 counts:", sum(data$absences != 0))) # 685
# y_bar <- barplot(table(data$absences))
print(paste('No. of rows =', length(data$absences)))
# Obs.
#   * Since variance >> mean, it's likely that data
#     is over-dispersed.
# Save cleaned data.
write.csv(data, file = "./data_clean.csv", row.names = FALSE)
